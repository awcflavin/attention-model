{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "ctoi = {c:i for i, c in enumerate(chars)}\n",
    "itoc = {i:c for c, i in ctoi.items()}\n",
    "encode = lambda x: [ctoi[c] for c in x]\n",
    "decode = lambda x: ''.join([itoc[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42442)\n",
    "# params\n",
    "vocab_size = len(chars)\n",
    "batch_size = 32 \n",
    "block_size = 64 # context length\n",
    "max_iters = 100000\n",
    "learning_rate = 4e-4\n",
    "emb_dim = 32\n",
    "num_head = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "Each character should have:\n",
    "1. a key - info about itself\n",
    "2. a query - something to match with previous characters\n",
    "3. a value - something thats output if theres matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42447)\n",
    "\n",
    "# self-attention head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
    "\n",
    "        # lower triangular mask keeps attention to past tokens only\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "\n",
    "        wei = q@k.transpose(-2, -1) * self.head_size**-0.5 # scaling factor to keep variance constant\n",
    "        # make all weights to future tokens to be -inf so softmax will give them 0.\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # [:T, :T] allows handling of variable sized context lengths\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # cat all head outputs\n",
    "        out = self.proj(out) # linear projection out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(nn.Linear(features, 4*features),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(4*features, features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, n_head):\n",
    "        super().__init__()\n",
    "        head_size = emb_dim // n_head\n",
    "        self.heads = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FF(emb_dim)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.heads(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42442)\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_embedding = nn.Embedding(block_size, emb_dim)\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(emb_dim, num_head),\n",
    "            TransformerBlock(emb_dim, num_head),\n",
    "            TransformerBlock(emb_dim, num_head),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.linear_out = nn.Linear(emb_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_embeddings = self.token_embedding(x) #  (Batch, Time, Channels=emb_dim) needs to be (Batch, Channels, Time) for F.cross_entropy\n",
    "        pos_embeddings = self.positional_embedding(torch.arange(T)) # (Time, Channels=emb_dim)\n",
    "        \n",
    "        x = tok_embeddings + pos_embeddings\n",
    "        x = self.blocks(x)\n",
    "        x= self.norm(x)\n",
    "        logits = self.linear_out(x) # (Batch, Time, Channels=vocab_size)\n",
    "\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # (Batch, Channels, Time)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, num_tokens=100):\n",
    "        for _ in range(num_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # last time step only - prediction\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2884745597839355\n",
      "2.2887392044067383\n",
      "2.153799295425415\n",
      "1.9790315628051758\n",
      "1.9709405899047852\n",
      "1.921905517578125\n",
      "1.8027561902999878\n",
      "1.7355912923812866\n",
      "1.6589696407318115\n",
      "1.7431864738464355\n",
      "1.6915522813796997\n",
      "1.667686104774475\n",
      "1.6288710832595825\n",
      "1.5517337322235107\n",
      "1.7228153944015503\n",
      "1.6182876825332642\n",
      "1.7119801044464111\n",
      "1.615836501121521\n",
      "1.6016979217529297\n",
      "1.6036126613616943\n",
      "1.618397831916809\n",
      "1.6096761226654053\n",
      "1.5946933031082153\n",
      "1.6630442142486572\n",
      "1.6356638669967651\n",
      "1.6423982381820679\n",
      "1.590080976486206\n",
      "1.6452021598815918\n",
      "1.642789602279663\n",
      "1.5645372867584229\n",
      "1.6224675178527832\n",
      "1.5617073774337769\n",
      "1.6046854257583618\n",
      "1.5925236940383911\n",
      "1.4771521091461182\n",
      "1.623162865638733\n",
      "1.5698789358139038\n",
      "1.5553843975067139\n",
      "1.506607174873352\n",
      "1.5822010040283203\n"
     ]
    }
   ],
   "source": [
    "m = LanguageModel()\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "for steps in range(max_iters):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 2500 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor([12,  0,  0,  ..., 45,  8,  0]): tensor(1.7774)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_val_loss():\n",
    "    eval_iters = 200\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(val)\n",
    "        logits, loss = m(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out[val] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "estimate_val_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I come torlow, good part in field but are grace,\n",
      "That both one, lord of that each:\n",
      "Aft I lod. Crivise and to to-jewle farewel:\n",
      "My came your marks up to toward, let's ration\n",
      "That estagers, nor a offent is and switer\n",
      "With rast the son promp; the to your noble his\n",
      "Brament, I'll years no make you his friend\n",
      "Octremites wishs s erring in made faps quint\n",
      "Should have made with And him nexessed home too\n",
      "Upound the plaugant Potas desir.\n",
      "Is that the far brial play this is bot?--\n",
      "Percitue father crand.\n",
      "\n",
      "Proth purness;\n",
      "Pooldom, my lord!\n",
      "Lord to that opes her shall the blood\n",
      "With him ne'e-wrepsitie streride becans,\n",
      "Or and my from and my will what we stones\n",
      "if I never God make within his find on.\n",
      "Ressure me addestagual, thousand lest we Clarence\n",
      "Trut yield of his my this back anway.\n",
      "\n",
      "LEONTES:\n",
      "Prick it young unto conce of the bestiancions.\n",
      "\n",
      "KING RICHARD ICI, find that in hath is my speak:\n",
      "let your cun Lord; thy speak wome the pute bed,\n",
      "The, and by unfring inciedinance!\n",
      "\n",
      "MENENIUS:\n",
      "Verman, by me to the prove I will not was at parting imity\n",
      "Shall warn bed their clacast as her once I rold,\n",
      "Is which the glood her wippsing, thy newlel,\n",
      "Shall, my body good Famthers 'tis soul.\n",
      "\n",
      "BENVOLIO:\n",
      "I have should pratch'd God sir, and let a betch thee!\n",
      "I'll from and a broke hath very.\n",
      "\n",
      "Bed,\n",
      "Sir, Vitaio, my quore than as blaren best the know\n",
      "Than no humbs were tell go strike\n",
      "A the noble. Hone that slen thou at your will vilace;\n",
      "Balic dish, go gives to us sloning, sir.\n",
      "Your the grief cluction this child, Pruse,\n",
      "O-mark. Pome oncomity Edward leave\n",
      "Out azoyal would day lies, that Frottle,\n",
      "Say a braw, what us love been thy hand make\n",
      "Bid ovird worminets for what prince,\n",
      "Hath powership and 'your to higage to is issixence\n",
      "'tis befalss not reparous hinorman,\n",
      "And a crive you for just being the strone\n",
      "That to her prophil.\n",
      "\n",
      "QUELIO:\n",
      "What uncrius hand make woman'st: Itelf, than reat.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "It't, then?\n",
      "\n",
      "CORIOLANUS:\n",
      "Pault, I'll am not safic that queenit anward!\n",
      "Has truke him, at to his proctres.\n",
      "\n",
      "First Clarencount my now.\n",
      "\n",
      "GLONTEN:\n",
      "Hest you?\n",
      "\n",
      "MATOLUS:\n",
      "Gy, 'tis neworn at yet shall I fare that thyself?\n",
      "\n",
      "ABETRES;\n",
      "Misshinest the bear a cousisfimine at can livina.\n",
      "\n",
      "Frovend:\n",
      "I she ported thy abundered and a maids with\n",
      "I even exily sward. Are with blingly soul\n",
      "toppeided eitfuls shall steeth neved and Romat\n",
      "That the bants, sweeter sat, as hand to frice;\n",
      "An moster the evee, a vitorain,\n",
      "That as low penofess; for my love,\n",
      "Wills had boing but mole be hast cause\n",
      "A I'll not ben, young to appow, To act world.\n",
      "\n",
      "Procamen:\n",
      "What had mut incall the know than hast the grates,\n",
      "K Ratese kindamm his kingnitaly,\n",
      "God us to should so, but leads tyre poor to about\n",
      "ussiness, father daught in thee!\n",
      "\n",
      "TYOLIZEL:\n",
      "O but the botter; as shrock: but blad, is my defitble\n",
      "Untern some sin therel canded fidere to pity.\n",
      "\n",
      "FLORIZNELE:\n",
      "Away's advalles.\n",
      "\n",
      "MENENIUS:\n",
      "Spay, may it the hath powers for is a sladiep\n",
      "With I can the hanke?\n",
      "Hath who more as motess plair's that mother\n",
      "Thy straike make usun heret me fall bly she't,\n",
      "Here me untim\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(context, 3000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
